{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "981e3646-c94f-4664-8a5f-d43a10cfecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "from src.dataset import ParityPredictionDataset, HiddenParityPrediction\n",
    "from src.model import TinyModel\n",
    "from src.train import train_model\n",
    "from src.plot import (\n",
    "    plot_losses,\n",
    "    plot_accuracies,\n",
    "    plot_line_with_label,\n",
    "    plot_list_of_lines_and_labels,\n",
    ")\n",
    "from src.common import get_accuracy_on_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b1ae0-62b5-4501-91a8-8ce678e0a297",
   "metadata": {},
   "source": [
    "## Finding the subnetwork after grokking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcc991-2cec-4ed2-88c4-5abff921eca3",
   "metadata": {},
   "source": [
    "First, we'll train the `TinyModel` network to get past the grokking phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490bb891-3a1c-4954-bd0f-eec849159dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 693\n",
      "Validation dataset size: 77\n",
      "Model initialised on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:26<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# first, we train the model to get grokked\n",
    "weight_decay = 1e-2\n",
    "learning_rate = 1e-1\n",
    "batch_size = 32\n",
    "hidden_size = 1000\n",
    "number_samples = 770\n",
    "epochs = 400\n",
    "\n",
    "# Replicability\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create the training dataset\n",
    "entire_dataset = HiddenParityPrediction(num_samples=number_samples, sequence_length=40, k=3)\n",
    "\n",
    "# Split into training and validation should be 1000 and 100\n",
    "train_size = int(0.90 * number_samples)\n",
    "val_size = number_samples - train_size\n",
    "training_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    entire_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training dataset size: {len(training_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "\n",
    "# Create the model\n",
    "model = TinyModel(\n",
    "    input_size=40,\n",
    "    hidden_layer_size=hidden_size,\n",
    "    output_size=1,\n",
    "    random_seed=0,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "(\n",
    "    model,\n",
    "    training_losses,\n",
    "    validation_losses,\n",
    "    training_accuracy,\n",
    "    validation_accuracy,\n",
    "    _,\n",
    ") = train_model(\n",
    "    training_dataset=training_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    model=model,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    loss_function_label=\"hinge\",\n",
    "    optimiser_function_label=\"sgd\",\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2fbab-3001-4f9d-bb23-c6e29c999911",
   "metadata": {},
   "source": [
    "Now, we want to find a subnetwork that contains the most active neurons. Recall that the theory from the original paper is that the generalisable networks are very sparse i.e. a small subnetwork is responsible for the bulk of predictive capability. To figure out how big this subnetwork is, we can conduct a search to determine the smallest subnetwork that produces the same predictions on the validation dataset as our larger network, to within some disagreement threshold. \n",
    "\n",
    "To do this, we'll zero-out certain neurons in the weight and bias layer. Let's write a function called `acc_calc` that takes in a dataset and a trained model, as well as a list of neurons to zero-out, and returns the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cf36910-8e4f-47dd-9838-79b0278150ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_calc(dataloader, model, masked_indices=None, device='cuda'):\n",
    "    \n",
    "    # if a mask is provided, create a copy of the model and apply the mask\n",
    "    if masked_indices is not None:\n",
    "        model = copy.deepcopy(model)\n",
    "        with torch.no_grad():\n",
    "            model.fc1.weight[masked_indices] = 0.\n",
    "            model.fc1.bias[masked_indices] = 0.\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    # evaluate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3348db7-2aaf-4f0d-befc-1af4227b4abb",
   "metadata": {},
   "source": [
    "Now, we need to write a function that employs binary search to find the minimum number of neurons required to achieve the original performance of a pre-trained model, based on the norms of the features. Let's break down how we're going to attack this:\n",
    "* The binary search process starts by initialising the left and right boundaries (left and right) of the search interval. Initially, left is set to 1, and right is set to the width of the first fully connected layer.\n",
    "* The variables `prev_k`, `min_k`, and `min_idx` are initialised. `prev_k` keeps track of the previous value of `k` (the number of neurons being evaluated). `min_k` and `min_idx` are used to store the minimum `k` value that recovers the original performance and the corresponding indices of the features.\n",
    "* The values of the `norms['feats']` array are sorted using `argsort()` and stored in the values variable. This array represents the norms of features at the final epoch.\n",
    "* The binary search loop continues until the left value is less than the right value.\n",
    "* Within each iteration of the loop, the middle point (`k`) between left and right is calculated using integer division.\n",
    "* If the current `k` value is the same as the previous `k` value (`prev_k`), indicating that the search is not progressing further, the loop is broken.\n",
    "* The `idx` variable is assigned the last `k` elements of the sorted values array, representing the indices of the top `k` features based on their norms.\n",
    "* The `acc_calc()` function is called twice to evaluate the accuracy of the `saved_model` on the given data loader. The first call uses the `idx` indices of the features, representing a masked version of the input, while the second call uses the full input.\n",
    "* If the accuracy obtained from the masked version is equal to the accuracy obtained from the full input, and the current `k` value is less than the current minimum `k` (`min_k`), the `min_k` and `min_idx` variables are updated.\n",
    "* Depending on the comparison between the masked accuracy and the full accuracy, the search interval (left and right) is adjusted. If the masked accuracy is lower, right is set to `k + 1`, otherwise left is set to `k`.\n",
    "* The current k value is assigned to `prev_k` to keep track of the previous value.\n",
    "\n",
    "Once the binary search loop finishes, the function returns the minimum `k` value (`min_k`) and the corresponding indices of the top features (`min_idx`) that recover the original performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec0e02c4-096e-40d0-94e6-c08bf0aa31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit_discovery_binary(saved_model, norms, dataloader, device='cuda'):\n",
    "    # Calculate least number of neurons that recovers original performance with binary search (assuming that it increases monotonically)\n",
    "    \n",
    "    width = saved_model.fc1.out_features\n",
    "    left, right = 1, width\n",
    "    prev_k = -1\n",
    "    min_k, min_idx = float('inf'), None\n",
    "\n",
    "    values = np.array(norms['feats']).argsort()\n",
    "    while left < right:\n",
    "        k = (left + right) // 2\n",
    "        if (prev_k == k):\n",
    "            break\n",
    "\n",
    "        idx = values[-k:]\n",
    "        masked_acc = acc_calc(dataloader, saved_model, idx, device=device)\n",
    "        full_acc = acc_calc(dataloader, saved_model, device=device)\n",
    "\n",
    "        if (masked_acc == full_acc) and (k < min_k):\n",
    "            min_k = k\n",
    "            min_idx = idx\n",
    "        if (masked_acc < full_acc):\n",
    "            left = k\n",
    "        else:\n",
    "            right = k + 1\n",
    "\n",
    "        prev_k = k\n",
    "    \n",
    "    return min_k, min_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8daf6a-e430-4ad0-8626-a852763f0b47",
   "metadata": {},
   "source": [
    "Finally, we can use the `find_smallest_subnetwork` function to search for the minimum set of neurons that give us the performance of the actual network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16aa4d28-0455-4973-9a85-f7fbd3128ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smallest_subnetwork(model, dataloader, threshold=0.01):\n",
    "    # Calculate the norm of the weights for each neuron in the trained model\n",
    "    norms = {'feats': torch.norm(model.fc1.weight.data, dim=1).cpu().numpy()}\n",
    "\n",
    "    # Find the smallest subnetwork that achieves the same performance\n",
    "    min_k, min_idx = circuit_discovery_binary(model, norms, dataloader)\n",
    "\n",
    "    print(f\"Smallest subnetwork size: {min_k}\")\n",
    "    return min_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8105ce14-63d7-4508-a1f3-6c6463974418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest subnetwork size: 2\n",
      "[ 98 306]\n"
     ]
    }
   ],
   "source": [
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, \n",
    "                                                shuffle=True, num_workers=4, pin_memory=True)\n",
    "min_idx = find_smallest_subnetwork(model, validation_loader, threshold=0.01)\n",
    "print(min_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a904350-7ded-4e4e-8828-163d96d2da0a",
   "metadata": {},
   "source": [
    "\"Smallest subnetwork size: 2\" means that the smallest subnetwork that produces the same predictions as the original network, within the defined threshold, consists of 2 neurons.\n",
    "\n",
    "The array `[98, 306]` represents the indices of these neurons in the hidden layer. These are the neurons that, according to the grokking process, seem to carry the most important features that allow the model to make accurate predictions. The process of finding the smallest subnetwork in this manner is a type of model simplification or \"pruning\", with the goal of reducing model complexity without significantly affecting performance.\n",
    "\n",
    "You can think of these neurons as the ones with the highest importance score according to the pruning algorithm. Let's potentially use this information to create a new, smaller network that only includes these neurons, which should be easier to interpret and less computationally intensive while still being capable of similarly accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9f100af-39ec-4b6b-b790-f9d5fbfd9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subnetwork(model, indices):\n",
    "    new_model = TinyModel(\n",
    "        model.input_size,\n",
    "        len(indices),  # the number of neurons in the hidden layer\n",
    "        model.output_size,\n",
    "    )\n",
    "\n",
    "    # Copy over the weights of the selected neurons\n",
    "    new_model.fc1.weight.data = model.fc1.weight.data[indices, :]\n",
    "    new_model.fc2.weight.data = model.fc2.weight.data[:, indices]\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21c41e-38cc-4c95-8b1d-7c642348c476",
   "metadata": {},
   "source": [
    "Now, let's create this subnetwork and use it for some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4490dda-0057-40f4-a4ce-3ae408a61643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised on device: cuda:0\n",
      "FULL MODEL PREDICTIONS\n",
      "--------------------------------------------------\n",
      "Predictions:  [ 1. -1.  1.  1.  1.]\n",
      "Targets:      [ 1. -1.  1.  1.  1.]\n",
      "Predictions:  [ 1. -1. -1.  1. -1.]\n",
      "Targets:      [ 1. -1. -1.  1. -1.]\n",
      "Predictions:  [ 1. -1. -1. -1.  1.]\n",
      "Targets:      [ 1. -1. -1. -1.  1.]\n",
      "\n",
      "\n",
      "SUBNETWORK PREDICTIONS\n",
      "--------------------------------------------------\n",
      "Predictions:  [0. 1. 1. 1. 1.]\n",
      "Targets:      [ 1.  1.  1.  1. -1.]\n",
      "Predictions:  [0. 1. 0. 0. 1.]\n",
      "Targets:      [ 1. -1.  1.  1. -1.]\n",
      "Predictions:  [1. 1. 1. 1. 1.]\n",
      "Targets:      [-1.  1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Create the subnetwork\n",
    "subnetwork = create_subnetwork(model, min_idx)\n",
    "subnetwork.to(model.device)\n",
    "\n",
    "print(\"FULL MODEL PREDICTIONS\")\n",
    "print(\"-\"*50)\n",
    "# Use the original model for predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs = inputs.to(model.device)\n",
    "        targets = targets.to(model.device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.sign(outputs).squeeze(-1) # Use sign to determine predicted class\n",
    "        print(\"Predictions: \", predicted.detach().cpu().numpy()[:5])\n",
    "        print(\"Targets:     \", targets.detach().cpu().numpy()[:5])\n",
    "print(\"\\n\")\n",
    "        \n",
    "print(\"SUBNETWORK PREDICTIONS\")\n",
    "print(\"-\"*50)\n",
    "# Use the subnetwork for predictions\n",
    "subnetwork.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs = inputs.to(subnetwork.device)\n",
    "        targets = targets.to(subnetwork.device)\n",
    "        outputs = subnetwork(inputs)\n",
    "        predicted = torch.sign(outputs).squeeze(-1) # Use sign to determine predicted class\n",
    "        print(\"Predictions: \", predicted.detach().cpu().numpy()[:5])\n",
    "        print(\"Targets:     \", targets.detach().cpu().numpy()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8e7b663-fd03-439b-8f2b-02e1094a8d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyModel(\n",
      "  (fc1): Linear(in_features=40, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(subnetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d000226d-1aac-4fc8-b2bb-6dd849e2ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyModel(\n",
      "  (fc1): Linear(in_features=40, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e6b87-23ee-4f91-9077-de4778b973ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
